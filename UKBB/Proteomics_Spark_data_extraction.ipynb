{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting proteomics data using Spark\n",
    "Spark is necessary because the dataset is large. Pipeline was adapted from\n",
    "https://github.com/dnanexus/UKB_RAP/blob/main/proteomics/0_extract_phenotype_protein_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Importing dependencies and preparing project/dataset IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# dxpy allows python to interact with the platform storage\n",
    "# Note: This notebook is using spark since the size of the dataset we're extracting\n",
    "# (i.e. the number of fields) is too large for a single node instance.\n",
    "import dxpy\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = \"/mnt/project/proteomics_results/\" # only read permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically discover dispensed dataset ID\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")\n",
    "dispensed_dataset_id = dispensed_dataset[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = dxpy.find_one_project()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = (\":\").join([project_id, dispensed_dataset_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: This cell can only be run once. Otherwise, you'll need to delete the existing data tables in order to re-run\n",
    "cmd = [\"dx\", \"extract_dataset\", dataset, \"-ddd\", \"--delimiter\", \",\"]\n",
    "subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting field names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dict_csv = glob.glob(os.path.join(path, \"*.data_dictionary.csv\"))[0]\n",
    "data_dict_df = pd.read_csv(data_dict_csv)\n",
    "data_dict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search for protein fields\n",
    "field_names = list(\n",
    "    data_dict_df.loc[data_dict_df[\"entity\"] == \"olink_instance_0\", \"name\"].values\n",
    ")\n",
    "print(len(field_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search for age and sex field names\n",
    "age_fields = list(\n",
    "    data_dict_df.loc[data_dict_df[\"name\"] == \"p21003_i0\", \"name\"].values # code for \"Age when attended assessment centre\", at baseline\n",
    ")\n",
    "\n",
    "age_fields\n",
    "\n",
    "# print(data_dict_df[data_dict_df['name'].str.contains(\"21003\")].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sex_fields = list(\n",
    "    data_dict_df.loc[data_dict_df[\"name\"] == \"p31\", \"name\"].values # code for \"Sex\"\n",
    ")\n",
    "\n",
    "sex_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_names_str = [f\"olink_instance_0.{f}\" for f in field_names]\n",
    "field_names_query = \",\".join(field_names_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export field name list to file for Table Exporter\n",
    "# Alternatively, instead of using dx extract_dataset you can use the Table exporter app\n",
    "# This list of field names can be used as input into the Table exporter app and then\n",
    "# you can ignore running the remaining cells in this notebook\n",
    "\n",
    "file = open('field_names.txt','w')\n",
    "for item in field_names:\n",
    "   file.write(item+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Need to adjust this buffer otherwise will get an error in toPandas() call\n",
    "conf = pyspark.SparkConf().set(\"spark.kryoserializer.buffer.max\", \"128m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"dx\",\n",
    "    \"extract_dataset\",\n",
    "    dataset,\n",
    "    \"--fields\",\n",
    "    field_names_query,\n",
    "    \"--delimiter\",\n",
    "    \",\",\n",
    "    \"--output\",\n",
    "    \"extracted_data.sql\",\n",
    "    \"--sql\",\n",
    "]\n",
    "subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"extracted_data.sql\", \"r\") as file:\n",
    "    retrieve_sql = \"\"\n",
    "    for line in file:\n",
    "        retrieve_sql += line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_df = spark.sql(retrieve_sql.strip(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = temp_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pdf.shape)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save as text file\n",
    "pdf.to_csv(\"complete_proteomics_df.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dx upload complete_proteomics_df.txt --destination proteomics_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dx ls -l proteomics_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting phenotype information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all field names\n",
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"dx\",\n",
    "    \"extract_dataset\",\n",
    "    dataset,\n",
    "    \"--list-fields\",\n",
    "]\n",
    "\n",
    "with open(\"all_fields.txt\", \"w\") as f:\n",
    "    subprocess.check_call(cmd, stdout=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Now, extract phenotype information\n",
    "field_names = [\"participant.eid\", \"participant.p21003_i0\", \"participant.p31\"]\n",
    "field_names_query = \",\".join(field_names)\n",
    "\n",
    "file = open('field_names.txt','w')\n",
    "for item in field_names:\n",
    "   file.write(item+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "!rm extracted_data2.sql\n",
    "\n",
    "# Need to adjust this buffer otherwise will get an error in toPandas() call\n",
    "# conf = pyspark.SparkConf().set(\"spark.kryoserializer.buffer.max\", \"128m\")\n",
    "\n",
    "# sc = pyspark.SparkContext(conf=conf)\n",
    "# spark = pyspark.sql.SparkSession(sc)\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "cmd = [\n",
    "    \"dx\",\n",
    "    \"extract_dataset\",\n",
    "    dataset,\n",
    "    \"--fields\",\n",
    "    field_names_query,\n",
    "    \"--delimiter\",\n",
    "    \",\",\n",
    "    \"--output\",\n",
    "    \"extracted_data2.sql\",\n",
    "    \"--sql\",\n",
    "]\n",
    "# cmd = [\n",
    "#     \"dx\", \"extract_dataset\", dataset,\n",
    "#     \"--fields\", \"participant.eid\",\n",
    "#     \"--fields\", \"participant.p21003_i0\",\n",
    "#     \"--fields\", \"participant.p31\",\n",
    "#     \"--delimiter\", \",\",\n",
    "#     \"--output\", \"extracted_data2.sql\",\n",
    "#     \"--sql\",\n",
    "# ]\n",
    "\n",
    "subprocess.check_call(cmd)\n",
    "\n",
    "with open(\"extracted_data2.sql\", \"r\") as file:\n",
    "    retrieve_sql = \"\"\n",
    "    for line in file:\n",
    "        retrieve_sql += line.strip()\n",
    "        \n",
    "temp_df = spark.sql(retrieve_sql.strip(\";\"))\n",
    "\n",
    "pdf = temp_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pdf.shape)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload all field names and phenotype information\n",
    "# Save as text file\n",
    "pdf.to_csv(\"age_sex_proteomics_df.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "!dx upload age_sex_proteomics_df.txt --destination proteomics_results/\n",
    "\n",
    "!dx upload all_fields.txt --destination proteomics_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dx ls -l proteomics_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
